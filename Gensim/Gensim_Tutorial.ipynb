{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpora and Vector Spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference : https://radimrehurek.com/gensim/tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder \"C:\\Users\\nsavi\\AppData\\Local\\Temp\" will be used to save temporary dictionary and corpus.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tempfile\n",
    "TEMP_FOLDER = tempfile.gettempdir()\n",
    "print('Folder \"{}\" will be used to save temporary dictionary and corpus.'.format(TEMP_FOLDER))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 15:26:29,883 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "             \"The EPS user interface management system\",\n",
    "             \"System and human system engineering testing of EPS\",              \n",
    "             \"Relation of user perceived response time to error measurement\",\n",
    "             \"The generation of random binary unordered trees\",\n",
    "             \"The intersection graph of paths in trees\",\n",
    "             \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "             \"Graph minors A survey\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a tiny corpus of nine documents, each consisting of only a single sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's tokenize the documents, remove common words(using a toy stoplist) as well as words that only appear once in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint #pretty-printer\n",
    "from collections import defaultdict\n",
    "\n",
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "#print(stoplist)\n",
    "\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "        for document in documents\n",
    "        ]\n",
    "#print(texts)\n",
    "\n",
    "#remove common words and tokenize\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "        \n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many ways of processing the documents, here we only split on workspace to tokenize, followed by lowercasing each word."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert documents to vectors, we\"ll use a document representation called **bag-of-words**. \\\n",
    "In this representation, each document is represented by one vector where a vector element i represents the number of times the ith word appears in the document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is advantageous to represent the questions only by their(integer) ids.\n",
    "The mapping between the questions and ids is called dictionary:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 15:26:29,913 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-22 15:26:29,914 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n",
      "2019-10-22 15:26:29,915 : INFO : saving Dictionary object under C:\\Users\\nsavi\\AppData\\Local\\Temp\\deerwester.dict, separately None\n",
      "2019-10-22 15:26:29,917 : INFO : saved C:\\Users\\nsavi\\AppData\\Local\\Temp\\deerwester.dict\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(texts)\n",
    "dictionary.save(os.path.join(TEMP_FOLDER, 'deerwester.dict'))  #store the dictionary, for future reference\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assigned unique integer ID to all words appearing in the processed corpus with the genism.corpora.dictionary.Dictionary class.\\\n",
    "This sweeps across the texts, collecting word counts and relevant statistics.\n",
    "In the end we see 12 distinct words in the processed corpus, which means each document will be represented by twelve numbers(ie., by a 12-D vector). To see the mapping between words and their ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 2, 0: 2, 4: 2, 7: 3, 5: 4, 3: 2, 6: 2, 8: 2, 9: 3, 10: 3, 11: 2}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.cfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 2, 2: 2, 0: 2, 4: 2, 7: 3, 5: 3, 3: 2, 6: 2, 8: 2, 9: 3, 10: 3, 11: 2}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.num_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.num_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.num_nnz)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To actually convert tokenized documents to vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function doc2bow() simply counts the number of occurances of each distint word, converts the word to its integer word id and returns the result as a bag-of-words-- a sparse vector, in  the form of [(word_id, word_count),....].\\\n",
    "\n",
    "As the token_id is 1 for \"human\" and 0 for \"computer\", the new document \"Human computer interaction\" will be transformed to [(1, 1), (0, 1)]. The words \"computer\" and \"human\" exist in the dictionary and appear once. Thus they become (1,1), (0, 1) repectively in the sparse vector. The word \"interaction\" doesn't exist in the dictionary and, thus, will not show up in the sparse vector. The other ten dictionary words, that appear(implicitly) zero times, will not show up in the sparse vector and there will never be a element in the sparse vector like (3,0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 15:26:29,991 : INFO : storing corpus in Matrix Market format to C:\\Users\\nsavi\\AppData\\Local\\Temp\\deerwester.mm\n",
      "2019-10-22 15:26:29,994 : INFO : saving sparse matrix to C:\\Users\\nsavi\\AppData\\Local\\Temp\\deerwester.mm\n",
      "2019-10-22 15:26:29,994 : INFO : PROGRESS: saving document #0\n",
      "2019-10-22 15:26:29,996 : INFO : saved 9x12 matrix, density=25.926% (28/108)\n",
      "2019-10-22 15:26:29,997 : INFO : saving MmCorpus index to C:\\Users\\nsavi\\AppData\\Local\\Temp\\deerwester.mm.index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'deerwester.mm'), corpus)\n",
    "for c in corpus:\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Streaming- One Document at a Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that corpus above resides fully in memory, as a plain Python list. In this simple example, it doesn't matter much, but just to make things clear, let's assume there are millions of documents in the corpus. Storing all of them in RAM won't do. Instead, let's assume the documents are stored in a file on disk, one document per line.\\\n",
    "Gensim only requires that a corpus be able to return one document vector at a time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from smart_open import smart_open\n",
    "class MyCorpus(object):\n",
    "    def __iter__(self):\n",
    "        for line in smart_open('datasets/mycorpus.txt', 'rb'):\n",
    "            # assume there is one document per line, tokens separated by whitespace\n",
    "            yield dictionary.doc2bow(line.lower().split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The assumption that each document occupies one line in a single file is not important; you can design the `__iter__` function to fit your input format.\n",
    "Just pass your input to retrive a clean list of tokens in each document, then convert the tokens via dictionary to their IDs and yeild the resulting sparse vector inside `__iter__`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<__main__.MyCorpus object at 0x109111D0>\n"
     ]
    }
   ],
   "source": [
    "corpus_memory_friendly = MyCorpus()  # doesn't load the corpus into memory!\n",
    "print(corpus_memory_friendly)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`corpus_memory_friendly` is now an object. We didn't define any way to `print` it, so print just outputs address of the object in memory. Not very useful. To see the constituent vectors, let's iterate over  the corpus and print each document vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (2, 1)]\n",
      "[(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)]\n",
      "[(2, 1), (5, 1), (7, 1), (8, 1)]\n",
      "[(1, 1), (5, 2), (8, 1)]\n",
      "[(3, 1), (6, 1), (7, 1)]\n",
      "[(9, 1)]\n",
      "[(9, 1), (10, 1)]\n",
      "[(9, 1), (10, 1), (11, 1)]\n",
      "[(4, 1), (10, 1), (11, 1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37-32\\lib\\site-packages\\smart_open\\smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "for vector in corpus_memory_friendly: # Load one vector into memory at a time\n",
    "    print(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although the output is the same as for the plain Python list, the corpus is now much more memory friendly, because at most one vector resides in RAM at a time. Now the corpus can now be as large as you want. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 16:05:03,583 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-22 16:05:03,586 : INFO : built Dictionary(42 unique tokens: ['abc', 'applications', 'computer', 'for', 'human']...) from 9 documents (total 69 corpus positions)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "from six import iteritems\n",
    "from smart_open import smart_open\n",
    "\n",
    "#collect statistics about all tokens\n",
    "dictionary = corpora.Dictionary(line.lower().split() for line in smart_open('datasets/mycorpus.txt', 'rb'))\n",
    "# print(dictionary)\n",
    "# print(type(dictionary))\n",
    "# remove stop words and words that appear only once\n",
    "stop_ids = [dictionary.token2id[stopword] for stopword in stoplist \n",
    "           if stopword in dictionary.token2id]\n",
    "once_ids = [tokenid for tokenid, docfreq in iteritems(dictionary.dfs) if docfreq == 1]\n",
    "\n",
    "# remove stop words and words that appear only once\n",
    "dictionary.filter_tokens(stop_ids + once_ids)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus Formats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exsists several file formats for serializing a Vector Space corpus(~sequence of vectors) to disk. Gensim implements then via the *streaming corpus interface* mentioned earlier: documents are read from(or stored to) disk in a lazy fashion, one document at a time, without the whole corpus being read into main memort at once."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more notable file formats is the __Matrix Market format__. To save a corpus in the Matrix Market format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 17:10:21,323 : INFO : storing corpus in Matrix Market format to C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.mm\n",
      "2019-10-22 17:10:21,325 : INFO : saving sparse matrix to C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.mm\n",
      "2019-10-22 17:10:21,326 : INFO : PROGRESS: saving document #0\n",
      "2019-10-22 17:10:21,328 : INFO : saved 2x2 matrix, density=25.000% (1/4)\n",
      "2019-10-22 17:10:21,330 : INFO : saving MmCorpus index to C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.mm.index\n"
     ]
    }
   ],
   "source": [
    "# create a toy corpus of 2 documents, as a plain Python list\n",
    "corpus = [[(1, 0.5)], []] # make one document empty, for the heck of it\n",
    "\n",
    "corpora.MmCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.mm'), corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 17:10:23,356 : INFO : converting corpus to SVMlight format: C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.svmlight\n",
      "2019-10-22 17:10:23,358 : INFO : saving SvmLightCorpus index to C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.svmlight.index\n",
      "2019-10-22 17:10:23,360 : INFO : no word id mapping provided; initializing from corpus\n",
      "2019-10-22 17:10:23,361 : INFO : storing corpus in Blei's LDA-C format into C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.lda-c\n",
      "2019-10-22 17:10:23,364 : INFO : saving vocabulary of 2 words to C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.lda-c.vocab\n",
      "2019-10-22 17:10:23,366 : INFO : saving BleiCorpus index to C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.lda-c.index\n",
      "2019-10-22 17:10:23,368 : INFO : no word id mapping provided; initializing from corpus\n",
      "2019-10-22 17:10:23,369 : INFO : storing corpus in List-Of-Words format into C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.low\n",
      "2019-10-22 17:10:23,371 : WARNING : List-of-words format can only save vectors with integer elements; 1 float entries were truncated to integer value\n",
      "2019-10-22 17:10:23,372 : INFO : saving LowCorpus index to C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.low.index\n"
     ]
    }
   ],
   "source": [
    "corpora.SvmLightCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.svmlight'), corpus)\n",
    "corpora.BleiCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.lda-c'), corpus)\n",
    "corpora.LowCorpus.serialize(os.path.join(TEMP_FOLDER, 'corpus.low'), corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversely, to load a corpus iterator from a Matrix Market file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 17:10:27,779 : INFO : loaded corpus index from C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.mm.index\n",
      "2019-10-22 17:10:27,780 : INFO : initializing corpus reader from C:\\Users\\nsavi\\AppData\\Local\\Temp\\corpus.mm\n",
      "2019-10-22 17:10:27,782 : INFO : accepted corpus with 2 documents, 2 features, 1 non-zero entries\n"
     ]
    }
   ],
   "source": [
    "corpus = corpora.MmCorpus(os.path.join(TEMP_FOLDER, 'corpus.mm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MmCorpus(2 documents, 2 features, 1 non-zero entries)\n"
     ]
    }
   ],
   "source": [
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(1, 0.5)], []]\n"
     ]
    }
   ],
   "source": [
    "# one way of printing a corpus: load it entirely into memory\n",
    "print(list(corpus))  # calling list() will convert any sequence to a plain Python list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another way of doing it: print one document at a time, making use of the streaming interface\n",
    "for doc in corpus:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compatibility with NumPy and SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gensim also contains efficient utility functions to help converting from/to '__numpy__' matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[8 1]\n",
      " [1 6]\n",
      " [5 6]\n",
      " [3 1]\n",
      " [9 7]]\n",
      "[[8. 1.]\n",
      " [1. 6.]\n",
      " [5. 6.]\n",
      " [3. 1.]\n",
      " [9. 7.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]\n",
      " [0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "numpy_matrix = np.random.randint(10, size=[5,2])\n",
    "print(numpy_matrix)\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "#print(corpus)\n",
    "numpy_matrix_dense = gensim.matutils.corpus2dense(corpus, num_terms=10)\n",
    "print(numpy_matrix_dense)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and from/to scipy.sparse  matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "scipy_sparse_matrix = scipy.sparse.random(5,2)\n",
    "print(scipy_sparse_matrix)\n",
    "corpus = gensim.matutils.Dense2Corpus(numpy_matrix)\n",
    "scipy_csc_matrix = gensim.matutils.corpus2csc(corpus)\n",
    "print(scipy_sparse_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Topics and Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To bring out hidden structure in the corpus, discover relationships between words and use them to describe the documents in a new and (hopefully) more semantic way.\n",
    "2. To make the document representation more compact. This both improves efficiency (new representation consumes less resources) and efficacy(marginal data trends are ignored , noise-reduction)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* First, we need to create a corpus to work with.\n",
    "* This step is the same as in the previous tutorial;\n",
    "* if you completed it, feel free to skip to the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora\n",
    "\n",
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove common words and tokenize\n",
    "\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n",
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 19:13:01,791 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-22 19:13:01,792 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# remove words that appear only once\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "        \n",
    "\n",
    "texts = [\n",
    "       [token for token in text if frequency[token] > 1]\n",
    "       for text in texts\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "#print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations are standard Python objects, typically initialized by means of a definition: `training corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 19:46:46,263 : INFO : collecting document frequencies\n",
      "2019-10-22 19:46:46,264 : INFO : PROGRESS: processing document #0\n",
      "2019-10-22 19:46:46,265 : INFO : calculating IDF weights for 9 documents and 12 features (28 matrix non-zeros)\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "tfidf = models.TfidfModel(corpus)  #step 1-- initialize a model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used our old corpus from tutorial 1 to initialise the transformational model. Different transformation may require different initialisation parameters; in case of TfIdf, the \"training\" consists simply of going through the supplied corpus once and computing document frequencies of all its features. Training other models, such as Latent Semantic Analysis or Latent Dirichlet Allocation,is much more involved and , cosequently , takes much more time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tranforming vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on, \"tfidf\" is treated as a read-only object that can be used to convert any vector from the old representation(bag-of-words integer counts) to the new representation. ( TfIdf real-valued weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.7071067811865476), (1, 0.7071067811865476)]\n"
     ]
    }
   ],
   "source": [
    "doc_bow = [(0,1), (1,1)]\n",
    "print(tfidf[doc_bow])   # step-2-- use the model to transform vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.5773502691896257), (1, 0.5773502691896257), (2, 0.5773502691896257)]\n",
      "[(0, 0.44424552527467476), (3, 0.44424552527467476), (4, 0.44424552527467476), (5, 0.3244870206138555), (6, 0.44424552527467476), (7, 0.3244870206138555)]\n",
      "[(2, 0.5710059809418182), (5, 0.4170757362022777), (7, 0.4170757362022777), (8, 0.5710059809418182)]\n",
      "[(1, 0.49182558987264147), (5, 0.7184811607083769), (8, 0.49182558987264147)]\n",
      "[(3, 0.6282580468670046), (6, 0.6282580468670046), (7, 0.45889394536615247)]\n",
      "[(9, 1.0)]\n",
      "[(9, 0.7071067811865475), (10, 0.7071067811865475)]\n",
      "[(9, 0.5080429008916749), (10, 0.5080429008916749), (11, 0.695546419520037)]\n",
      "[(4, 0.6282580468670046), (10, 0.45889394536615247), (11, 0.6282580468670046)]\n"
     ]
    }
   ],
   "source": [
    "# Or to apply a transformation to a whole corpus\n",
    "\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this particular case, we are transforming the same corpus that we used for training, but this is only incidental. Once the transformation model has been initialised, it can be used on any vectors (provided they come from the same vector space, of course), even if they are not used in the training corpus at all. This is achived by a process called folding-in for LSA, by topic inference for LDA etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformations can also be serialised, one on top of another, in sort of chain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 20:41:29,776 : INFO : using serial LSI version on this node\n",
      "2019-10-22 20:41:29,777 : INFO : updating model with new documents\n",
      "2019-10-22 20:41:29,778 : INFO : preparing a new chunk of documents\n",
      "2019-10-22 20:41:29,779 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-10-22 20:41:29,780 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2019-10-22 20:41:29,781 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2019-10-22 20:41:29,784 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2019-10-22 20:41:29,785 : INFO : computing the final decomposition\n",
      "2019-10-22 20:41:29,786 : INFO : keeping 2 factors (discarding 47.565% of energy spectrum)\n",
      "2019-10-22 20:41:29,787 : INFO : processed documents up to #9\n",
      "2019-10-22 20:41:29,788 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2019-10-22 20:41:29,789 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LsiModel(num_terms=12, num_topics=2, decay=1.0, chunksize=20000)\n",
      "<gensim.interfaces.TransformedCorpus object at 0x109B01F0>\n"
     ]
    }
   ],
   "source": [
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=2)   #initialize an LSI tranformation\n",
    "print(lsi)\n",
    "\n",
    "corpus_lsi = lsi[corpus_tfidf]  # create a double wrapper over the original corpus\n",
    "print(corpus_lsi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we transformed our Tf-Idf corpus via `Latent Semantic Indexing` into a latent 2D space. Now you're probably wondering: what do these two latent dimensions stand for? Let's inspect with: func: `models.LsiModel.print_topics`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 20:45:17,458 : INFO : topic #0(1.594): 0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"\n",
      "2019-10-22 20:45:17,459 : INFO : topic #1(1.476): -0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.703*\"trees\" + 0.538*\"graph\" + 0.402*\"minors\" + 0.187*\"survey\" + 0.061*\"system\" + 0.060*\"time\" + 0.060*\"response\" + 0.058*\"user\" + 0.049*\"computer\" + 0.035*\"interface\"'),\n",
       " (1,\n",
       "  '-0.460*\"system\" + -0.373*\"user\" + -0.332*\"eps\" + -0.328*\"interface\" + -0.320*\"time\" + -0.320*\"response\" + -0.293*\"computer\" + -0.280*\"human\" + -0.171*\"survey\" + 0.161*\"trees\"')]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsi.print_topics(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that according to LSI, \"trees\", \"graph\" and \"minors\" are all related words(and contribute most to the direction of the first topic), while the second topic practically concerns itself with all the other words, As expected, the first five documents and more strongly related to the second topic while the remaining four documents to the first topic:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " both bow->tfidf and tfidf -> lsi transformation are actually executed here, on the fly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.06600783396090704), (1, -0.5200703306361848)] Human machine interface for lab abc computer applications\n",
      "[(0, 0.19667592859142993), (1, -0.7609563167700033)] A survey of user opinion of computer system response time\n",
      "[(0, 0.0899263997244685), (1, -0.7241860626752508)] The EPS user interface management system\n",
      "[(0, 0.07585847652178529), (1, -0.6320551586003424)] System and human system engineering testing of EPS\n",
      "[(0, 0.10150299184980505), (1, -0.5737308483002945)] Relation of user perceived response time to error measurement\n",
      "[(0, 0.7032108939378302), (1, 0.16115180214026184)] The generation of random binary unordered trees\n",
      "[(0, 0.8774787673119822), (1, 0.16758906864659903)] The intersection graph of paths in trees\n",
      "[(0, 0.9098624686818569), (1, 0.14086553628719517)] Graph minors IV Widths of trees and well quasi ordering\n",
      "[(0, 0.6165825350569283), (1, -0.05392907566389018)] Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "for doc, as_text in zip(corpus_lsi, documents):\n",
    "    print(doc, as_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 21:00:42,386 : INFO : saving Projection object under model.lsi.projection, separately None\n",
      "2019-10-22 21:00:42,389 : INFO : saved model.lsi.projection\n",
      "2019-10-22 21:00:42,391 : INFO : saving LsiModel object under model.lsi, separately None\n",
      "2019-10-22 21:00:42,392 : INFO : not storing attribute projection\n",
      "2019-10-22 21:00:42,392 : INFO : not storing attribute dispatcher\n",
      "2019-10-22 21:00:42,395 : INFO : saved model.lsi\n",
      "2019-10-22 21:00:42,396 : INFO : loading LsiModel object from model.lsi\n",
      "2019-10-22 21:00:42,398 : INFO : loading id2word recursively from model.lsi.id2word.* with mmap=None\n",
      "2019-10-22 21:00:42,400 : INFO : setting ignored attribute projection to None\n",
      "2019-10-22 21:00:42,401 : INFO : setting ignored attribute dispatcher to None\n",
      "2019-10-22 21:00:42,401 : INFO : loaded model.lsi\n",
      "2019-10-22 21:00:42,402 : INFO : loading LsiModel object from model.lsi.projection\n",
      "2019-10-22 21:00:42,404 : INFO : loaded model.lsi.projection\n"
     ]
    }
   ],
   "source": [
    "# Model persistency is achieved with the :func:`save` and :func:`load` functions:\n",
    "\n",
    "lsi.save('model.lsi')  # same for tfidf, lda, ...\n",
    "lsi = models.LsiModel.load('model.lsi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next question might be: just how exactly similar are those documents to each other?\n",
    "Is there a way to formalise the similarity, so that for a given input document, we can order some other set of documents according to their similarity?\n",
    "Similarly queries are covered in the next tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Available Transformation\n",
    "\n",
    "1. Term Frequency\n",
    "2. Latent Semantic Indexing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrates querying a corpus for similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the Corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to create a corpus to work with.\n",
    "This step is the same as in the previous tutorial.\n",
    "if you completed it, feel free to skip to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 21:19:04,056 : INFO : adding document #0 to Dictionary(0 unique tokens: [])\n",
      "2019-10-22 21:19:04,058 : INFO : built Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...) from 9 documents (total 29 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity interface\n",
    "\n",
    "We covered what it means to create a corpus in the Vector Space Model and how to transform it between different vector spaces.\n",
    "A common reason for such a charade is that we want to determine **similarity between pairs of documents**, or the **similarity between a specific document and a set of other documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 21:23:16,442 : INFO : using serial LSI version on this node\n",
      "2019-10-22 21:23:16,443 : INFO : updating model with new documents\n",
      "2019-10-22 21:23:16,444 : INFO : preparing a new chunk of documents\n",
      "2019-10-22 21:23:16,445 : INFO : using 100 extra samples and 2 power iterations\n",
      "2019-10-22 21:23:16,447 : INFO : 1st phase: constructing (12, 102) action matrix\n",
      "2019-10-22 21:23:16,448 : INFO : orthonormalizing (12, 102) action matrix\n",
      "2019-10-22 21:23:16,451 : INFO : 2nd phase: running dense svd on (12, 9) matrix\n",
      "2019-10-22 21:23:16,452 : INFO : computing the final decomposition\n",
      "2019-10-22 21:23:16,452 : INFO : keeping 2 factors (discarding 43.156% of energy spectrum)\n",
      "2019-10-22 21:23:16,453 : INFO : processed documents up to #9\n",
      "2019-10-22 21:23:16,454 : INFO : topic #0(3.341): 0.644*\"system\" + 0.404*\"user\" + 0.301*\"eps\" + 0.265*\"response\" + 0.265*\"time\" + 0.240*\"computer\" + 0.221*\"human\" + 0.206*\"survey\" + 0.198*\"interface\" + 0.036*\"graph\"\n",
      "2019-10-22 21:23:16,455 : INFO : topic #1(2.542): 0.623*\"graph\" + 0.490*\"trees\" + 0.451*\"minors\" + 0.274*\"survey\" + -0.167*\"system\" + -0.141*\"eps\" + -0.113*\"human\" + 0.107*\"response\" + 0.107*\"time\" + -0.072*\"interface\"\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "lsi = models.LsiModel(corpus, id2word=dictionary, num_topics=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now suppose a user typed in the query `\"Human computer interaction\"`. We would\n",
    " like to sort our nine corpus documents in decreasing order of relevance to this query.\n",
    " Unlike modern search engines, here we only concentrate on a single aspect of possible\n",
    " similarities---on apparent semantic relatedness of their texts (words). No hyperlinks,\n",
    " no random-walk static ranks, just a semantic extension over the boolean keyword match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.4618210045327159), (1, -0.07002766527899912)]\n"
     ]
    }
   ],
   "source": [
    "doc = \"Human computer interaction\"\n",
    "vec_bow = dictionary.doc2bow(doc.lower().split())\n",
    "vec_lsi = lsi[vec_bow]  # convert the query to LSI space\n",
    "print(vec_lsi)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 21:24:53,653 : WARNING : scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "2019-10-22 21:24:53,655 : INFO : creating matrix with 9 documents and 2 features\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(lsi[corpus])  # transform corpus to LSI space and index it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-10-22 21:26:05,597 : INFO : saving MatrixSimilarity object under deerwester.index, separately None\n",
      "2019-10-22 21:26:05,599 : INFO : saved deerwester.index\n",
      "2019-10-22 21:26:05,601 : INFO : loading MatrixSimilarity object from deerwester.index\n",
      "2019-10-22 21:26:05,603 : INFO : loaded deerwester.index\n"
     ]
    }
   ],
   "source": [
    "index.save('deerwester.index')\n",
    "index = similarities.MatrixSimilarity.load('deerwester.index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.998093), (1, 0.93748635), (2, 0.9984453), (3, 0.9865886), (4, 0.90755945), (5, -0.12416792), (6, -0.10639259), (7, -0.09879464), (8, 0.050041765)]\n"
     ]
    }
   ],
   "source": [
    "# Performing queries\n",
    "# ++++++++++++++++++\n",
    "#\n",
    "# To obtain similarities of our query document against the nine indexed documents:\n",
    "\n",
    "sims = index[vec_lsi]  # perform a similarity query against the corpus\n",
    "print(list(enumerate(sims)))  # print (document_number, document_similarity) 2-tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 0.9984453) Human machine interface for lab abc computer applications\n",
      "(0, 0.998093) A survey of user opinion of computer system response time\n",
      "(3, 0.9865886) The EPS user interface management system\n",
      "(1, 0.93748635) System and human system engineering testing of EPS\n",
      "(4, 0.90755945) Relation of user perceived response time to error measurement\n",
      "(8, 0.050041765) The generation of random binary unordered trees\n",
      "(7, -0.09879464) The intersection graph of paths in trees\n",
      "(6, -0.10639259) Graph minors IV Widths of trees and well quasi ordering\n",
      "(5, -0.12416792) Graph minors A survey\n"
     ]
    }
   ],
   "source": [
    "sims = sorted(enumerate(sims), key=lambda item: -item[1])\n",
    "for i, s in enumerate(sims):\n",
    "    print(s, documents[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
